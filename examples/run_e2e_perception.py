"""
End-to-End Multi-Task Perception Demo
æ¼”ç¤ºå®Œæ•´çš„ç«¯åˆ°ç«¯å¤šä»»åŠ¡æ„ŸçŸ¥ï¼šæ£€æµ‹ + åˆ†å‰² + è·Ÿè¸ª
"""
import torch
import numpy as np
import argparse
import os
from pathlib import Path
from mmcv import Config
from mmdet3d.models import build_model
from mmdet3d.datasets import build_dataset, build_dataloader


def parse_args():
    parser = argparse.ArgumentParser(description='E2E Multi-Task Perception')
    parser.add_argument('config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('checkpoint', help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--out-dir', default='output/e2e_perception', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', default='cuda:0', help='ä½¿ç”¨çš„è®¾å¤‡')
    parser.add_argument('--visualize', action='store_true', help='æ˜¯å¦å¯è§†åŒ–')
    parser.add_argument('--save-results', action='store_true', help='æ˜¯å¦ä¿å­˜ç»“æœ')
    parser.add_argument('--num-samples', type=int, default=10, help='å¤„ç†æ ·æœ¬æ•°')
    return parser.parse_args()


def load_model(config_path, checkpoint_path, device='cuda:0'):
    """åŠ è½½æ¨¡å‹"""
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")
    cfg = Config.fromfile(config_path)
    
    model = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg'))
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    model.load_state_dict(state_dict, strict=False)
    model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model, cfg


def run_inference(model, data, device='cuda:0'):
    """è¿è¡Œæ¨ç†"""
    with torch.no_grad():
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        for key in data:
            if isinstance(data[key], torch.Tensor):
                data[key] = data[key].to(device)
        
        # å‰å‘ä¼ æ’­
        results = model(**data)
    
    return results


def print_results_summary(results):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“Š æ„ŸçŸ¥ç»“æœæ‘˜è¦")
    print("="*60)
    
    for idx, result in enumerate(results):
        print(f"\næ ·æœ¬ {idx+1}:")
        
        # 1. æ£€æµ‹ç»Ÿè®¡
        if 'boxes_3d' in result:
            boxes = result['boxes_3d']
            scores = result['scores_3d']
            labels = result['labels_3d']
            
            print(f"  ğŸ¯ æ£€æµ‹: {len(boxes)} ä¸ªç›®æ ‡")
            if len(boxes) > 0:
                print(f"     å¹³å‡ç½®ä¿¡åº¦: {scores.mean():.3f}")
                print(f"     ç±»åˆ«åˆ†å¸ƒ: {np.bincount(labels.numpy())}")
        
        # 2. åˆ†å‰²ç»Ÿè®¡
        if 'seg_mask' in result:
            seg_mask = result['seg_mask']
            print(f"  ğŸ—ºï¸  åˆ†å‰²: {seg_mask.shape[0]} ä¸ªç±»åˆ«")
            for cls_id in range(seg_mask.shape[0]):
                mask = seg_mask[cls_id] > 0.5
                ratio = mask.float().mean().item()
                print(f"     ç±»åˆ«{cls_id}: {ratio*100:.1f}% åƒç´ ")
        
        # 3. è·Ÿè¸ªç»Ÿè®¡
        if 'track_ids' in result and result['track_ids'] is not None:
            track_ids = result['track_ids']
            unique_ids = torch.unique(track_ids)
            print(f"  ğŸ¬ è·Ÿè¸ª: {len(unique_ids)} ä¸ªè½¨è¿¹")
        
        # 4. è½¨è¿¹é¢„æµ‹
        if 'trajectories' in result and result['trajectories'] is not None:
            trajs = result['trajectories']
            print(f"  ğŸ”® è½¨è¿¹: é¢„æµ‹æœªæ¥ {trajs.shape[1]} å¸§")
    
    print("="*60 + "\n")


def save_results(results, save_dir):
    """ä¿å­˜ç»“æœ"""
    os.makedirs(save_dir, exist_ok=True)
    
    for idx, result in enumerate(results):
        save_path = os.path.join(save_dir, f'result_{idx:04d}.npz')
        
        # è½¬æ¢ä¸ºnumpy
        result_np = {}
        for key, value in result.items():
            if value is not None:
                if torch.is_tensor(value):
                    result_np[key] = value.cpu().numpy()
                else:
                    result_np[key] = value
        
        np.savez(save_path, **result_np)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")


def visualize_results(model, results, save_dir=None):
    """å¯è§†åŒ–ç»“æœ"""
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–...")
    model.visualize_predictions(results, save_dir)
    print(f"âœ… å¯è§†åŒ–å®Œæˆ")


def analyze_perception_quality(results):
    """åˆ†ææ„ŸçŸ¥è´¨é‡"""
    print("\n" + "="*60)
    print("ğŸ“ˆ æ„ŸçŸ¥è´¨é‡åˆ†æ")
    print("="*60)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_detections = 0
    high_conf_detections = 0
    total_seg_pixels = 0
    total_tracks = set()
    
    for result in results:
        # æ£€æµ‹è´¨é‡
        if 'boxes_3d' in result:
            boxes = result['boxes_3d']
            scores = result['scores_3d']
            total_detections += len(boxes)
            high_conf_detections += (scores > 0.5).sum().item()
        
        # åˆ†å‰²è´¨é‡
        if 'seg_mask' in result:
            seg_mask = result['seg_mask']
            total_seg_pixels += (seg_mask > 0.5).sum().item()
        
        # è·Ÿè¸ªè´¨é‡
        if 'track_ids' in result and result['track_ids'] is not None:
            track_ids = result['track_ids'].cpu().numpy()
            total_tracks.update(track_ids)
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\næ£€æµ‹è´¨é‡:")
    print(f"  æ€»æ£€æµ‹æ•°: {total_detections}")
    print(f"  é«˜ç½®ä¿¡åº¦æ£€æµ‹ (>0.5): {high_conf_detections}")
    print(f"  é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {high_conf_detections/max(total_detections,1)*100:.1f}%")
    
    print(f"\nåˆ†å‰²è´¨é‡:")
    print(f"  åˆ†å‰²åƒç´ æ€»æ•°: {total_seg_pixels}")
    
    print(f"\nè·Ÿè¸ªè´¨é‡:")
    print(f"  å”¯ä¸€è½¨è¿¹æ•°: {len(total_tracks)}")
    
    print("="*60 + "\n")


def benchmark_performance(model, data_loader, device, num_samples=10):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "="*60)
    print("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*60)
    
    import time
    
    model.eval()
    
    # é¢„çƒ­
    print("é¢„çƒ­ä¸­...")
    for i, data in enumerate(data_loader):
        if i >= 3:
            break
        _ = run_inference(model, data, device)
    
    # æµ‹è¯•
    print("æµ‹è¯•ä¸­...")
    times = []
    
    for i, data in enumerate(data_loader):
        if i >= num_samples:
            break
        
        torch.cuda.synchronize()
        start = time.time()
        
        _ = run_inference(model, data, device)
        
        torch.cuda.synchronize()
        end = time.time()
        
        times.append(end - start)
        print(f"  æ ·æœ¬ {i+1}/{num_samples}: {times[-1]*1000:.2f} ms")
    
    # ç»Ÿè®¡
    times = np.array(times)
    print(f"\næ€§èƒ½ç»Ÿè®¡:")
    print(f"  å¹³å‡æ—¶é—´: {times.mean()*1000:.2f} ms")
    print(f"  ä¸­ä½æ•°: {np.median(times)*1000:.2f} ms")
    print(f"  æ ‡å‡†å·®: {times.std()*1000:.2f} ms")
    print(f"  FPS: {1.0/times.mean():.2f}")
    
    print("="*60 + "\n")


def extract_multi_task_features(model, data, device='cuda:0'):
    """æå–å¤šä»»åŠ¡ç‰¹å¾"""
    print("\nğŸ” æå–å¤šä»»åŠ¡ç‰¹å¾...")
    
    with torch.no_grad():
        for key in data:
            if isinstance(data[key], torch.Tensor):
                data[key] = data[key].to(device)
        
        features = model.extract_multi_task_features(**data)
    
    print("âœ… ç‰¹å¾æå–å®Œæˆ")
    print(f"\nç‰¹å¾ä¿¡æ¯:")
    for key, value in features.items():
        if torch.is_tensor(value):
            print(f"  {key:30s}: {list(value.shape)}")
    
    return features


def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.out_dir, exist_ok=True)
    
    print("="*60)
    print("ğŸš€ BEVFusion ç«¯åˆ°ç«¯å¤šä»»åŠ¡æ„ŸçŸ¥ç³»ç»Ÿ")
    print("="*60)
    
    # 1. åŠ è½½æ¨¡å‹
    model, cfg = load_model(args.config, args.checkpoint, args.device)
    
    # 2. å‡†å¤‡æ•°æ®
    print("\nğŸ“¦ å‡†å¤‡æ•°æ®...")
    dataset = build_dataset(cfg.data.test)
    data_loader = build_dataloader(
        dataset,
        samples_per_gpu=1,
        workers_per_gpu=0,
        dist=False,
        shuffle=False
    )
    print(f"âœ… æ•°æ®é›†: {len(dataset)} ä¸ªæ ·æœ¬")
    
    # 3. è¿è¡Œæ¨ç†
    print(f"\nâš™ï¸  è¿è¡Œæ¨ç† (å¤„ç† {args.num_samples} ä¸ªæ ·æœ¬)...")
    all_results = []
    
    for i, data in enumerate(data_loader):
        if i >= args.num_samples:
            break
        
        print(f"  å¤„ç†æ ·æœ¬ {i+1}/{args.num_samples}...")
        results = run_inference(model, data, args.device)
        all_results.extend(results)
    
    print("âœ… æ¨ç†å®Œæˆ")
    
    # 4. æ‰“å°ç»“æœæ‘˜è¦
    print_results_summary(all_results)
    
    # 5. åˆ†ææ„ŸçŸ¥è´¨é‡
    analyze_perception_quality(all_results)
    
    # 6. ä¿å­˜ç»“æœ
    if args.save_results:
        results_dir = os.path.join(args.out_dir, 'results')
        save_results(all_results, results_dir)
    
    # 7. å¯è§†åŒ–
    if args.visualize:
        vis_dir = os.path.join(args.out_dir, 'visualizations')
        visualize_results(model, all_results, vis_dir)
    
    # 8. æ€§èƒ½æµ‹è¯•
    print("\næ˜¯å¦è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•? (y/n): ", end='')
    if input().lower() == 'y':
        benchmark_performance(model, data_loader, args.device, min(args.num_samples, 10))
    
    # 9. ç‰¹å¾æå–ç¤ºä¾‹
    print("\næ˜¯å¦æå–å¤šä»»åŠ¡ç‰¹å¾? (y/n): ", end='')
    if input().lower() == 'y':
        data = next(iter(data_loader))
        features = extract_multi_task_features(model, data, args.device)
        
        # ä¿å­˜ç‰¹å¾
        if args.save_results:
            feat_dir = os.path.join(args.out_dir, 'features')
            os.makedirs(feat_dir, exist_ok=True)
            
            features_np = {}
            for key, value in features.items():
                if torch.is_tensor(value) and value is not None:
                    features_np[key] = value.cpu().numpy()
            
            np.savez(os.path.join(feat_dir, 'multi_task_features.npz'), **features_np)
            print(f"âœ… ç‰¹å¾å·²ä¿å­˜")
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.out_dir}")
    print("="*60)


if __name__ == '__main__':
    main()
